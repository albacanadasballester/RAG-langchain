{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG\n",
    "\n",
    "This notebook is a fully setup basic RAG app including\n",
    "\n",
    "- PDF Loader\n",
    "- Chunking\n",
    "- Vector Embedding\n",
    "- Pinecone Vector Database\n",
    "- Retrieval\n",
    "- Question/Response\n",
    "\n",
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create virtual environment\n",
    "! python -m venv venv\n",
    "! source venv/bin/activate\n",
    "! which python #make sure that the end of your path output contains \"...venv/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all packages\n",
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "`(1) Packages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load all environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the environment variables\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_endpoint = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "## LLM\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "## Pinecone Vector Database\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "pinecone_api_host = os.getenv('PINECONE_API_HOST')\n",
    "index_name = os.getenv('PINECONE_INDEX_NAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) LangSmith`\n",
    "\n",
    "https://docs.smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = langchain_tracing_v2\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = langchain_endpoint\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) API Keys`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "openai_model = \"gpt-3.5-turbo\"\n",
    "\n",
    "#Pinecone keys\n",
    "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
    "os.environ['PINECONE_API_HOST'] = pinecone_api_host\n",
    "os.environ['PINECONE_INDEX_NAME'] = index_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) Pinecone Init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "'''client = chromadb.PersistentClient(\n",
    "    path=\"test\",\n",
    "    settings=Settings(),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "'''\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"192.168.107.42\",\n",
    "    port=8000,\n",
    "    ssl=False,\n",
    "    headers=None\n",
    ")\n",
    "\n",
    "index = client.get_or_create_collection(\"rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "index = pc.Index(os.environ['PINECONE_INDEX_NAME'])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full RAG App (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_HOST = '192.168.107.42'\n",
    "OLLAMA_PORT = '11434'\n",
    "OLLAMA_URL = f\"http://{OLLAMA_HOST}:{OLLAMA_PORT}\"\n",
    "\n",
    "OLLAMA_MODEL= 'llama3.1'\n",
    "OLLAMA_NUM_THREADS= 8\n",
    "OLLAMA_TEMP=0\n",
    "HUGGINGFACE_EMBEDDING = 'sentence-transformers/all-mpnet-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/bRAG-langchain/.venvrag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "#from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "pdf_file_paths = \"test/scikit-dataset-transformations-feature-extraction.pdf\"\n",
    "loader = PyPDFLoader(pdf_file_paths)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Embed\n",
    "'''vectorstore = Chroma.from_documents(client=client,\n",
    "    documents=splits, \n",
    "    embedding=embeddings,\n",
    "    collection_name='test01'\n",
    ")'''\n",
    "\n",
    "# Usar Chroma como almac√©n vectorial\n",
    "vectorstore = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"rag\",\n",
    "    embedding_function=embeddings\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(\n",
    "  base_url=OLLAMA_URL,\n",
    "  model=OLLAMA_MODEL,\n",
    "  num_thread=OLLAMA_NUM_THREADS,\n",
    "  temperature=OLLAMA_TEMP\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided configuration, here are the active features:\n",
      "\n",
      "1. **VPC (Virtual Port Channel)**:\n",
      "\t* `vpc 4` on interface port-channel5\n",
      "\t* `vpc 5` on interface port-channel5\n",
      "\t* `vpc 15` on interface port-channel15\n",
      "\t* `vpc 100` on interface port-channel100\n",
      "2. **FEX (Fabric Extender)**:\n",
      "\t* `fex associate 101` on interface port-channel15\n",
      "3. **VSAN (Virtual Storage Area Network)**:\n",
      "\t* `vsan database` with multiple VSANs (10) and interfaces associated with them\n",
      "4. **Port Channels**:\n",
      "\t* Port channel5 is active, with a description of \"mq1-ucshyfx-b\"\n",
      "\t* Port channel10 is active, with a description of \"mq-fas2552-a - Uplink Ethernet\"\n",
      "\t* Port channel15 is active, with a description of \"Uplink Corporativo - mq1-mgsw-1\"\n",
      "\t* Port channel100 is active, with a description of \"Uplink Corporativo - mq1-mgsw-1\"\n",
      "5. **Trunking**:\n",
      "\t* Trunking is enabled on multiple interfaces (e.g., Ethernet1/1, Ethernet1/2, port-channel5)\n",
      "6. **VFC (Virtual Fabric Channel)**:\n",
      "\t* Multiple VFCs are bound to physical interfaces (e.g., vfc7, vfc11, vfc12)\n",
      "\n",
      "Note that this list only includes features that are explicitly mentioned in the configuration as being active or enabled.\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "print(rag_chain.invoke(\"List active features in the configuration device mq1-n5k-1?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "**     Acuntia Maqueta Sistemas  **\n",
      "**            mq1-n5k-1          **\n",
      "***********************************\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"Show me banner motd in mq1-n5k-1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "**     Acuntia Maqueta Sistemas  **\n",
      "**            mq1-n5k-1          **\n",
      "***********************************\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"Show me banner motd in mq1-n5k-2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
